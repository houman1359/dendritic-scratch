{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mi_binned(y, c, n_bins=10, vmin=-3.0, vmax=3.0, n_classes=10):\n",
    "    \"\"\"\n",
    "    Returns I(y; c) by naive histogram binning.\n",
    "    We do NOT attempt a real gradient wrt y here\n",
    "    (the gradient is effectively zero except on bin boundaries).\n",
    "    \"\"\"\n",
    "    B = y.size(0)\n",
    "    device = y.device\n",
    "    \n",
    "    edges = torch.linspace(vmin, vmax, n_bins+1, device=device)\n",
    "    bin_idx = torch.bucketize(y, edges) - 1\n",
    "    bin_idx = torch.clamp(bin_idx, 0, n_bins-1)\n",
    "    \n",
    "    joint_counts = torch.zeros(n_bins, n_classes, device=device)\n",
    "    for i in range(B):\n",
    "        b = bin_idx[i].item()\n",
    "        cc = c[i].item()\n",
    "        joint_counts[b, cc] += 1\n",
    "    joint_prob = joint_counts / float(B)\n",
    "    \n",
    "    p_bin = joint_prob.sum(dim=1, keepdim=True)  # (n_bins,1)\n",
    "    p_c   = joint_prob.sum(dim=0, keepdim=True)  # (1,n_classes)\n",
    "    mask = (joint_prob > 0)\n",
    "    p_bc = joint_prob[mask]\n",
    "    \n",
    "    bin_idxs, class_idxs = mask.nonzero(as_tuple=True)\n",
    "    p_b = p_bin[bin_idxs]\n",
    "    p_cc = p_c[0, class_idxs]\n",
    "    \n",
    "    mi_val = (p_bc * (torch.log(p_bc) - torch.log(p_b) - torch.log(p_cc))).sum()\n",
    "    return mi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLayerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A 'layer' with K scalar neurons: y_i = w_i^T x + b_i, i=1..K.\n",
    "    We'll do local updates for each neuron (though we store them in one linear).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, K=10):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, K, bias=True)  # W shape: (K, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, in_dim)\n",
    "        # Y: (B, K)\n",
    "        Y = self.linear(x)\n",
    "        return Y\n",
    "    \n",
    "    def compute_redundancy(self, i, j):\n",
    "        \"\"\"\n",
    "        Dot product of w_i and w_j for i != j.\n",
    "        \"\"\"\n",
    "        W = self.linear.weight  # shape: (K, in_dim)\n",
    "        wi = W[i,:]\n",
    "        wj = W[j,:]\n",
    "        return torch.dot(wi, wj)\n",
    "    \n",
    "    def forward_neuron(self, x, i):\n",
    "        \"\"\"\n",
    "        Return the scalar y_i for the i-th neuron only.\n",
    "        \"\"\"\n",
    "        # y_i = w_i^T x + b_i\n",
    "        # We can do a slice of the linear layer\n",
    "        W = self.linear.weight[i,:].unsqueeze(0)   # shape (1, in_dim)\n",
    "        b = self.linear.bias[i].unsqueeze(0)       # shape (1,)\n",
    "        # forward => y shape (B,1)\n",
    "        y = torch.matmul(x, W.t()) + b\n",
    "        return y.squeeze(1)  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_update_layer(\n",
    "    layer_block,\n",
    "    train_loader,\n",
    "    lr=1e-3,\n",
    "    lambda_reg=0.01,\n",
    "    epochs=3,\n",
    "    n_bins=10,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    A 'local' update rule:\n",
    "      For each mini-batch (X,c):\n",
    "        For i in [0..K-1]:\n",
    "          1) y_i = w_i^T X + b_i\n",
    "          2) L_i = -I(y_i; c) + lambda * sum_{j != i} dot(w_i, w_j)\n",
    "          3) compute grad(L_i) w.r.t. w_i, update w_i only\n",
    "    \"\"\"\n",
    "    layer_block.to(device)\n",
    "    optimizer = torch.optim.SGD(layer_block.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        layer_block.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for X, labels in train_loader:\n",
    "            X = X.view(X.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            K_ = layer_block.linear.weight.shape[0]\n",
    "            \n",
    "            # We'll do one pass for each neuron i\n",
    "            for i in range(K_):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # compute y_i\n",
    "                y_i = layer_block.forward_neuron(X, i)  # shape (B,)\n",
    "                \n",
    "                # estimate I(y_i;c)\n",
    "                mi_i = estimate_mi_binned(y_i, labels, n_bins=n_bins)\n",
    "                \n",
    "                # redundancy term: sum_{j != i} w_i dot w_j\n",
    "                # we can read w_i from layer_block.linear.weight[i,:]\n",
    "                # let's do a quick sum\n",
    "                w_i = layer_block.linear.weight[i,:]\n",
    "                red_sum = 0.0\n",
    "                for j in range(K_):\n",
    "                    if j != i:\n",
    "                        w_j = layer_block.linear.weight[j,:]\n",
    "                        red_sum += torch.dot(w_i, w_j)\n",
    "                \n",
    "                # loss_i = -mi_i + lambda * red_sum\n",
    "                # Minimizing => we want to minimize negative MI => maximizing MI\n",
    "                # plus the redundancy penalty\n",
    "                loss_i = -mi_i + lambda_reg * red_sum\n",
    "                \n",
    "                # local backward pass \n",
    "                # By default, PyTorch will compute grad for *all* parameters\n",
    "                loss_i.backward(retain_graph=True)\n",
    "                \n",
    "                # Now we want to \"zero out\" gradient for weights that are not w_i or b_i\n",
    "                # Let's do that:\n",
    "                with torch.no_grad():\n",
    "                    for name, param in layer_block.named_parameters():\n",
    "                        if 'weight' in name:\n",
    "                            # param shape (K, in_dim)\n",
    "                            # zero out rows != i\n",
    "                            grad_ = param.grad\n",
    "                            if grad_ is not None:\n",
    "                                for row in range(K_):\n",
    "                                    if row != i:\n",
    "                                        grad_[row,:] = 0.0\n",
    "                        elif 'bias' in name:\n",
    "                            # param shape (K,)\n",
    "                            grad_ = param.grad\n",
    "                            if grad_ is not None:\n",
    "                                for row in range(K_):\n",
    "                                    if row != i:\n",
    "                                        grad_[row] = 0.0\n",
    "                \n",
    "                # take an update step \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss_i.item()\n",
    "        \n",
    "        # average loss over #batches*K\n",
    "        num_updates = len(train_loader) * K_\n",
    "        avg_loss = total_loss / float(num_updates)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Avg Local Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return layer_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def get_hidden_representation(layer_block, data_loader):\n",
    "    layer_block.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.view(images.size(0), -1).to(device)\n",
    "            Y = layer_block(images)  # (B, K)\n",
    "            all_feats.append(Y.cpu())\n",
    "            all_labels.append(labels)\n",
    "    return torch.cat(all_feats, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "def build_and_train_2layer_local_mi(train_loader, test_loader, device=device):\n",
    "    # 1) Create layer1\n",
    "    layer1 = LocalLayerBlock(in_dim=784, K=16)\n",
    "    # local training\n",
    "    layer1 = local_update_layer(layer1, train_loader, lr=1e-3, lambda_reg=0.01, epochs=2, n_bins=10, device=device)\n",
    "    \n",
    "    # 2) Freeze\n",
    "    freeze_module(layer1)\n",
    "    \n",
    "    # 3) Extract hidden rep\n",
    "    X_train_hid, y_train = get_hidden_representation(layer1, train_loader)\n",
    "    X_test_hid, y_test   = get_hidden_representation(layer1, test_loader)\n",
    "    \n",
    "    # Make new data loaders\n",
    "    train_hid_ds = torch.utils.data.TensorDataset(X_train_hid, y_train)\n",
    "    test_hid_ds  = torch.utils.data.TensorDataset(X_test_hid, y_test)\n",
    "    train_hid_loader = torch.utils.data.DataLoader(train_hid_ds, batch_size=128, shuffle=True)\n",
    "    test_hid_loader  = torch.utils.data.DataLoader(test_hid_ds, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # 4) Create layer2\n",
    "    layer2 = LocalLayerBlock(in_dim=16, K=16)\n",
    "    # local training\n",
    "    layer2 = local_update_layer(layer2, train_hid_loader, lr=1e-3, lambda_reg=0.01, epochs=2, n_bins=10, device=device)\n",
    "    \n",
    "    return layer1, layer2, train_hid_loader, test_hid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, in_dim=16, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def evaluate_final_representation(layer2, train_loader, test_loader):\n",
    "    freeze_module(layer2)\n",
    "    \n",
    "    # Extract final hidden\n",
    "    X_train_final, y_train = get_hidden_representation(layer2, train_loader)\n",
    "    X_test_final,  y_test  = get_hidden_representation(layer2, test_loader)\n",
    "    \n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_final, y_train)\n",
    "    test_ds  = torch.utils.data.TensorDataset(X_test_final, y_test)\n",
    "    \n",
    "    train_loader2 = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    test_loader2  = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Train a standard linear classifier\n",
    "    clf = FinalClassifier(in_dim=X_train_final.shape[1], num_classes=10).to(device)\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        clf.train()\n",
    "        total_loss = 0.0\n",
    "        for feats, labs in train_loader2:\n",
    "            feats = feats.to(device)\n",
    "            labs = labs.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = clf(feats)\n",
    "            loss = ce_loss(logits, labs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader2)\n",
    "        print(f\"Linear Classifier Epoch {epoch+1}/5 - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    clf.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for feats, labs in test_loader2:\n",
    "            feats = feats.to(device)\n",
    "            labs = labs.to(device)\n",
    "            logits = clf(feats)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labs).sum().item()\n",
    "            total += labs.size(0)\n",
    "    acc = 100.0 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer1_local, layer2_local, train_hid_loader, test_hid_loader \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_and_train_2layer_local_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m acc_local \u001b[38;5;241m=\u001b[39m evaluate_final_representation(layer2_local, train_hid_loader, test_hid_loader)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal test accuracy (local MI rule): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_local\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mbuild_and_train_2layer_local_mi\u001b[0;34m(train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m LocalLayerBlock(in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# local training\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_update_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 2) Freeze\u001b[39;00m\n\u001b[1;32m     24\u001b[0m freeze_module(layer1)\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mlocal_update_layer\u001b[0;34m(layer_block, train_loader, lr, lambda_reg, epochs, n_bins, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_i \u001b[38;5;241m=\u001b[39m layer_block\u001b[38;5;241m.\u001b[39mforward_neuron(X, i)  \u001b[38;5;66;03m# shape (B,)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# estimate I(y_i;c)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mi_i \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_mi_binned\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# redundancy term: sum_{j != i} w_i dot w_j\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# we can read w_i from layer_block.linear.weight[i,:]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# let's do a quick sum\u001b[39;00m\n\u001b[1;32m     44\u001b[0m w_i \u001b[38;5;241m=\u001b[39m layer_block\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mweight[i,:]\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mestimate_mi_binned\u001b[0;34m(y, c, n_bins, vmin, vmax, n_classes)\u001b[0m\n\u001b[1;32m     14\u001b[0m joint_counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n_bins, n_classes, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[0;32m---> 16\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43mbin_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     cc \u001b[38;5;241m=\u001b[39m c[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m     joint_counts[b, cc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer1_local, layer2_local, train_hid_loader, test_hid_loader = build_and_train_2layer_local_mi(train_loader, test_loader, device=device)\n",
    "acc_local = evaluate_final_representation(layer2_local, train_hid_loader, test_hid_loader)\n",
    "print(f\"Final test accuracy (local MI rule): {acc_local:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_backprop(model, train_loader, lr=1e-3, epochs=5, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = ce_loss(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.view(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bp = TwoLayerNet(in_dim=784, hidden_dim=64, num_classes=10)\n",
    "train_backprop(model_bp, train_loader, lr=1e-3, epochs=5, device=device)\n",
    "\n",
    "test_acc_bp = evaluate_accuracy(model_bp, test_loader, device=device)\n",
    "print(f\"Standard Backprop 2-layer test accuracy: {test_acc_bp:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dendritic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
