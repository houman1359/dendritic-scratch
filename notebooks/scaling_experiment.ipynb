{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('scaling_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 12:30:01,080 - scaling_notebook - INFO - Added /n/holylabs/LABS/kempner_dev/Users/hsafaai/Code/dendritic-modeling to Python path\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path if needed\n",
    "current_dir = os.getcwd()\n",
    "if not any(p.endswith('dendritic_modeling') for p in sys.path):\n",
    "    if os.path.basename(current_dir) == 'notebooks':\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added {project_root} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the repo\n",
    "from dendritic_modeling.config import load_config\n",
    "from dendritic_modeling.models import ProbabilisticClassifier, Classifier\n",
    "from dendritic_modeling.networks import ExcitationInhibitionNetwork, MLPExcInhNetwork\n",
    "from dendritic_modeling.synthetic_datasets import get_unified_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and modify a configuration file\n",
    "def load_config_yaml(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def prepare_config(base_config_path, dim, net_type=\"EINet\", epochs=5):\n",
    "    config = load_config(base_config_path)\n",
    "    \n",
    "    # Set network type\n",
    "    config.model.network.type = net_type\n",
    "    \n",
    "    # Set dimensions according to the sweep parameter\n",
    "    if net_type == \"EINet\":\n",
    "        config.model.network.parameters.excitatory_branch_factors = [2, int(dim)]\n",
    "        config.model.network.parameters.inhibitory_branch_factors = []\n",
    "    elif net_type == \"MLP\":\n",
    "        config.model.network.parameters.hidden_layer_sizes = [int(dim), int(dim//2)]\n",
    "    \n",
    "    # Set training parameters\n",
    "    config.train.epochs = epochs\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize model based on configuration\n",
    "def initialize_model(model_cfg):\n",
    "    task = model_cfg.task\n",
    "    probabilistic = model_cfg.probabilistic\n",
    "    net_type = model_cfg.network.type\n",
    "    net_params = model_cfg.network.parameters.__dict__\n",
    "    \n",
    "    if task == 'classification':\n",
    "        if net_type == 'MLP':\n",
    "            logger.info(f\"Creating MLP network\")\n",
    "            net = MLPExcInhNetwork(**net_params)\n",
    "            output_dim = net_params.get('output_dim', 10)\n",
    "            if probabilistic:\n",
    "                return ProbabilisticClassifier(net, output_dim)\n",
    "            else:\n",
    "                return Classifier(net)\n",
    "        elif net_type == 'EINet':\n",
    "            logger.info(f\"Creating EINet network\")\n",
    "            net = ExcitationInhibitionNetwork(**net_params)\n",
    "            output_dim = net_params['excitatory_layer_sizes'][-1]\n",
    "            if probabilistic:\n",
    "                return ProbabilisticClassifier(net, output_dim)\n",
    "            else:\n",
    "                return Classifier(net)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid network type: {net_type}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            if hasattr(model, 'compute_loss'):\n",
    "                loss = model.compute_loss(inputs, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if hasattr(model, 'predict'):\n",
    "                predictions = model.predict(inputs)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "            \n",
    "            total += targets.size(0)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a single experiment and return results\n",
    "def run_single_experiment(base_config_path, dim, net_type=\"EINet\", epochs=5):\n",
    "    config = prepare_config(base_config_path, dim, net_type, epochs)\n",
    "    \n",
    "    # Get datasets\n",
    "    train_ds, valid_ds, test_ds = get_unified_datasets(config.task, config.train)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = initialize_model(config.model)\n",
    "    \n",
    "    # Count parameters\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"{net_type} dim={dim}: Parameter count = {param_count}\")\n",
    "    \n",
    "    # Move to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Evaluate model (without training - for quick analysis)\n",
    "    accuracy, loss = evaluate_model(model, test_loader, device)\n",
    "    logger.info(f\"{net_type} dim={dim}: Accuracy = {accuracy:.4f}, Loss = {loss:.4f}\")\n",
    "    \n",
    "    return {\"dim\": dim, \"param_count\": param_count, \"accuracy\": accuracy, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a full sweep of experiments\n",
    "def run_scaling_experiment(base_config_path, dim_list, net_types=[\"EINet\", \"MLP\"], epochs=5):\n",
    "    results = {net_type: [] for net_type in net_types}\n",
    "    \n",
    "    for net_type in net_types:\n",
    "        for dim in dim_list:\n",
    "            result = run_single_experiment(base_config_path, dim, net_type, epochs)\n",
    "            results[net_type].append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot results on log-log scale\n",
    "def plot_loglog(results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    # Colors and markers for different network types\n",
    "    styles = {\n",
    "        \"EINet\": {\"color\": \"blue\", \"marker\": \"o\"},\n",
    "        \"MLP\": {\"color\": \"red\", \"marker\": \"s\"}\n",
    "    }\n",
    "    \n",
    "    for net_type, style in styles.items():\n",
    "        if net_type not in results or not results[net_type]:\n",
    "            logger.warning(f\"No data for {net_type}\")\n",
    "            continue\n",
    "        \n",
    "        data_list = sorted(results[net_type], key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        x_param = [d[\"param_count\"] for d in data_list]\n",
    "        y_loss = [max(1e-10, d[\"loss\"]) for d in data_list]  # Avoid log(0)\n",
    "        y_error = [max(1e-10, 1.0 - d[\"accuracy\"]) for d in data_list]  # Convert accuracy to error rate\n",
    "        dims = [d[\"dim\"] for d in data_list]\n",
    "        \n",
    "        # Plot loss curve\n",
    "        ax1.plot(x_param, y_loss, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                 label=f\"{net_type} (Loss)\", linewidth=2, markersize=10)\n",
    "        \n",
    "        # Plot error rate curve\n",
    "        ax2.plot(x_param, y_error, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                linestyle='--', label=f\"{net_type} (Error)\", linewidth=2, markersize=10)\n",
    "        \n",
    "        for i, (x, y1, y2, dim) in enumerate(zip(x_param, y_loss, y_error, dims)):\n",
    "            ax1.annotate(f\"dim={dim}\", xy=(x, y1), xytext=(10, 0),\n",
    "                        textcoords='offset points', fontsize=10)\n",
    "            ax2.annotate(f\"dim={dim}\", xy=(x, y2), xytext=(10, 0),\n",
    "                        textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    # Set labels and titles\n",
    "    ax1.set_xlabel(\"Parameter Count (log scale)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Loss (log scale)\", fontsize=12)\n",
    "    ax1.set_title(\"Loss vs Parameter Count\", fontsize=14)\n",
    "    ax1.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax1.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    ax2.set_xlabel(\"Parameter Count (log scale)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Error Rate (log scale)\", fontsize=12)\n",
    "    ax2.set_title(\"Error Rate vs Parameter Count\", fontsize=14)\n",
    "    ax2.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax2.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate scaling exponents\n",
    "def calculate_scaling_exponents(results):\n",
    "    scaling_data = {}\n",
    "    \n",
    "    for net_type in [\"EINet\", \"MLP\"]:\n",
    "        if net_type not in results or not results[net_type]:\n",
    "            print(f\"No data for {net_type}\")\n",
    "            continue\n",
    "            \n",
    "        data_list = sorted(results[net_type], key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        if len(data_list) < 2:\n",
    "            print(f\"Not enough data points for {net_type} to calculate scaling exponent\")\n",
    "            continue\n",
    "            \n",
    "        log_params = np.log(np.array([d[\"param_count\"] for d in data_list]))\n",
    "        log_loss = np.log(np.array([max(1e-10, d[\"loss\"]) for d in data_list]))\n",
    "        log_error = np.log(np.array([max(1e-10, 1.0 - d[\"accuracy\"]) for d in data_list]))\n",
    "        \n",
    "        loss_slope, loss_intercept = np.polyfit(log_params, log_loss, 1)\n",
    "        error_slope, error_intercept = np.polyfit(log_params, log_error, 1)\n",
    "        \n",
    "        scaling_data[net_type] = {\n",
    "            \"loss_slope\": loss_slope,\n",
    "            \"error_slope\": error_slope,\n",
    "            \"loss_intercept\": loss_intercept,\n",
    "            \"error_intercept\": error_intercept\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{net_type} Scaling:\")\n",
    "        print(f\"  Loss scaling exponent: {loss_slope:.4f}\")\n",
    "        print(f\"  Error scaling exponent: {error_slope:.4f}\")\n",
    "        \n",
    "        if loss_slope < 0:\n",
    "            print(f\"  Loss scales with parameters as: loss ∝ (params)^{loss_slope:.4f}\")\n",
    "            print(f\"  Doubling parameter count decreases loss by {2**abs(loss_slope)-1:.2%}\")\n",
    "        \n",
    "        if error_slope < 0:\n",
    "            print(f\"  Error scales with parameters as: error ∝ (params)^{error_slope:.4f}\")\n",
    "            print(f\"  Doubling parameter count decreases error by {2**abs(error_slope)-1:.2%}\")\n",
    "    \n",
    "    return scaling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize scaling laws with fitted lines\n",
    "def plot_scaling_laws(results, scaling_data):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    styles = {\n",
    "        \"EINet\": {\"color\": \"blue\", \"marker\": \"o\"},\n",
    "        \"MLP\": {\"color\": \"red\", \"marker\": \"s\"}\n",
    "    }\n",
    "    \n",
    "    for net_type, style in styles.items():\n",
    "        if net_type not in results or not results[net_type]:\n",
    "            continue\n",
    "        \n",
    "        data_list = sorted(results[net_type], key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        x_param = np.array([d[\"param_count\"] for d in data_list])\n",
    "        y_loss = np.array([max(1e-10, d[\"loss\"]) for d in data_list])\n",
    "        y_error = np.array([max(1e-10, 1.0 - d[\"accuracy\"]) for d in data_list])\n",
    "        \n",
    "        # Plot data points\n",
    "        ax1.scatter(x_param, y_loss, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                   s=80, label=f\"{net_type} Data\")\n",
    "        ax2.scatter(x_param, y_error, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                   s=80, label=f\"{net_type} Data\")\n",
    "        \n",
    "        if net_type in scaling_data:\n",
    "            # Plot fitted lines\n",
    "            x_range = np.logspace(np.log10(min(x_param)), np.log10(max(x_param)), 100)\n",
    "            \n",
    "            loss_slope = scaling_data[net_type][\"loss_slope\"]\n",
    "            loss_intercept = scaling_data[net_type][\"loss_intercept\"]\n",
    "            y_loss_fit = np.exp(loss_intercept) * x_range**loss_slope\n",
    "            \n",
    "            error_slope = scaling_data[net_type][\"error_slope\"] \n",
    "            error_intercept = scaling_data[net_type][\"error_intercept\"]\n",
    "            y_error_fit = np.exp(error_intercept) * x_range**error_slope\n",
    "            \n",
    "            ax1.plot(x_range, y_loss_fit, '--', color=style[\"color\"], \n",
    "                    label=f\"{net_type} Fit: ∝ N^{loss_slope:.3f}\")\n",
    "            ax2.plot(x_range, y_error_fit, '--', color=style[\"color\"], \n",
    "                    label=f\"{net_type} Fit: ∝ N^{error_slope:.3f}\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Parameter Count (N)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=12)\n",
    "    ax1.set_title(\"Scaling Law: Loss vs Parameters\", fontsize=14)\n",
    "    ax1.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax1.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    ax2.set_xlabel(\"Parameter Count (N)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Error Rate\", fontsize=12)\n",
    "    ax2.set_title(\"Scaling Law: Error Rate vs Parameters\", fontsize=14)\n",
    "    ax2.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax2.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 12:30:01,696 - scaling_notebook - INFO - Creating EINet network\n",
      "2025-02-25 12:30:01,700 - scaling_notebook - INFO - EINet dim=2: Parameter count = 72210\n"
     ]
    }
   ],
   "source": [
    "# Main execution code to run the sweep\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration \n",
    "    BASE_CONFIG = \"../.vscode/config_exp.yaml\"  # Update path as needed\n",
    "    DIM_LIST = [2, 4, 8, 16, 32]  # Dimensions to sweep\n",
    "    NET_TYPES = [\"EINet\", \"MLP\"]  # Network types to test\n",
    "    EPOCHS = 5  # Epochs for each run (set low for quick testing)\n",
    "    \n",
    "    # Run the experiment\n",
    "    results = run_scaling_experiment(\n",
    "        base_config_path=BASE_CONFIG,\n",
    "        dim_list=DIM_LIST,\n",
    "        net_types=NET_TYPES,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    for net_type, data_list in results.items():\n",
    "        print(f\"\\n{net_type} Results:\")\n",
    "        for data in data_list:\n",
    "            print(f\"  dim={data['dim']}: params={data['param_count']}, \"\n",
    "                  f\"accuracy={data['accuracy']:.4f}, loss={data['loss']:.4f}\")\n",
    "    \n",
    "    # Create basic plot\n",
    "    plot_figure = plot_loglog(results)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and plot scaling exponents\n",
    "    scaling_data = calculate_scaling_exponents(results)\n",
    "    scaling_figure = plot_scaling_laws(results, scaling_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
