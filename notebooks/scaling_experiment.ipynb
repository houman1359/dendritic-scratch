{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('scaling_notebook')\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if not any(p.endswith('dendritic_modeling') for p in sys.path):\n",
    "    if os.path.basename(current_dir) == 'notebooks':\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added {project_root} to Python path\")\n",
    "\n",
    "try:\n",
    "    from dendritic_modeling.config import load_config\n",
    "    from dendritic_modeling.models import ProbabilisticClassifier, Classifier\n",
    "    from dendritic_modeling.networks import ExcitationInhibitionNetwork, MLPExcInhNetwork\n",
    "    from dendritic_modeling.synthetic_datasets import get_unified_datasets\n",
    "    logger.info(\"Successfully imported necessary modules\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Failed to import: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_yaml(path):\n",
    "    \"\"\"Load a YAML configuration file.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def initialize_model(model_cfg):\n",
    "    \"\"\"Build the specified model.\"\"\"\n",
    "    task = model_cfg.task\n",
    "    probabilistic = model_cfg.probabilistic\n",
    "    net_type = model_cfg.network.type\n",
    "    net_params = model_cfg.network.parameters.__dict__\n",
    "    \n",
    "    if task == 'classification':\n",
    "        if net_type == 'MLP':\n",
    "            logger.info(f\"Creating MLP network\")\n",
    "            net = MLPExcInhNetwork(**net_params)\n",
    "            output_dim = net_params.get('output_dim', 10)\n",
    "            if probabilistic:\n",
    "                return ProbabilisticClassifier(net, output_dim)\n",
    "            else:\n",
    "                return Classifier(net)\n",
    "        elif net_type == 'EINet':\n",
    "            logger.info(f\"Creating EINet network\")\n",
    "            net = ExcitationInhibitionNetwork(**net_params)\n",
    "            output_dim = net_params['excitatory_layer_sizes'][-1]\n",
    "            if probabilistic:\n",
    "                return ProbabilisticClassifier(net, output_dim)\n",
    "            else:\n",
    "                return Classifier(net)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid network type: {net_type}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task: {task}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model accuracy and loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            if hasattr(model, 'compute_loss'):\n",
    "                loss = model.compute_loss(inputs, targets)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if hasattr(model, 'predict'):\n",
    "                predictions = model.predict(inputs)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "            \n",
    "            total += targets.size(0)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(base_config_path, dim, net_type=\"EINet\", epochs=20):\n",
    "    \"\"\"Run a single experiment with the given dimension and network type.\"\"\"\n",
    "    config = load_config(base_config_path)\n",
    "    \n",
    "    config.model.network.type = net_type\n",
    "    \n",
    "    config.model.network.parameters.excitatory_branch_factors = [2, int(dim)]\n",
    "    config.model.network.parameters.inhibitory_branch_factors = []\n",
    "    \n",
    "    config.train.epochs = epochs\n",
    "    \n",
    "    train_ds, valid_ds, test_ds = get_unified_datasets(config.task, config.train)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64)\n",
    "    \n",
    "    model = initialize_model(config.model)\n",
    "    \n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"{net_type} dim={dim}: Parameter count = {param_count}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    accuracy, loss = evaluate_model(model, test_loader, device)\n",
    "    logger.info(f\"{net_type} dim={dim}: Accuracy = {accuracy:.4f}, Loss = {loss:.4f}\")\n",
    "    \n",
    "    return param_count, accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = \"../.vscode/config_exp.yaml\"  \n",
    "DIM_LIST = [2, 4, 8, 16, 32]  # Dimensions to sweep\n",
    "EPOCHS = 2  \n",
    "results = run_scaling_experiment(BASE_CONFIG, DIM_LIST, epochs=EPOCHS)\n",
    "\n",
    "for net_type, data_list in results.items():\n",
    "    print(f\"\\n{net_type} Results:\")\n",
    "    for data in data_list:\n",
    "        print(f\"  dim={data['dim']}: params={data['param_count']}, \"\n",
    "              f\"accuracy={data['accuracy']:.4f}, loss={data['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglog(results):\n",
    "    \"\"\"Create log-log plots for parameter count vs loss and accuracy.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    # Colors and markers for different network types\n",
    "    styles = {\n",
    "        \"EINet\": {\"color\": \"blue\", \"marker\": \"o\"},\n",
    "        \"MLP\": {\"color\": \"red\", \"marker\": \"s\"}\n",
    "    }\n",
    "    \n",
    "    for net_type, style in styles.items():\n",
    "        if net_type not in results or not results[net_type]:\n",
    "            logger.warning(f\"No data for {net_type}\")\n",
    "            continue\n",
    "        \n",
    "        data_list = sorted(results[net_type], key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        x_param = [d[\"param_count\"] for d in data_list]\n",
    "        y_loss = [max(1e-10, d[\"loss\"]) for d in data_list]  # Avoid log(0)\n",
    "        y_error = [max(1e-10, 1.0 - d[\"accuracy\"]) for d in data_list]  # Convert accuracy to error rate\n",
    "        dims = [d[\"dim\"] for d in data_list]\n",
    "        \n",
    "        # Plot loss curve\n",
    "        ax1.plot(x_param, y_loss, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                 label=f\"{net_type} (Loss)\", linewidth=2, markersize=10)\n",
    "        \n",
    "        # Plot error rate curve\n",
    "        ax2.plot(x_param, y_error, marker=style[\"marker\"], color=style[\"color\"], \n",
    "                linestyle='--', label=f\"{net_type} (Error)\", linewidth=2, markersize=10)\n",
    "        \n",
    "        for i, (x, y1, y2, dim) in enumerate(zip(x_param, y_loss, y_error, dims)):\n",
    "            ax1.annotate(f\"dim={dim}\", xy=(x, y1), xytext=(10, 0),\n",
    "                        textcoords='offset points', fontsize=10)\n",
    "            ax2.annotate(f\"dim={dim}\", xy=(x, y2), xytext=(10, 0),\n",
    "                        textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    # Set labels and titles\n",
    "    ax1.set_xlabel(\"Parameter Count (log scale)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Loss (log scale)\", fontsize=12)\n",
    "    ax1.set_title(\"Loss vs Parameter Count\", fontsize=14)\n",
    "    ax1.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax1.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    ax2.set_xlabel(\"Parameter Count (log scale)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Error Rate (log scale)\", fontsize=12)\n",
    "    ax2.set_title(\"Error Rate vs Parameter Count\", fontsize=14)\n",
    "    ax2.grid(True, which=\"both\", alpha=0.3)\n",
    "    ax2.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot the results\n",
    "plot_figure = plot_loglog(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scaling_exponents(results):\n",
    "    \"\"\"Calculate the scaling exponents for loss and error rate vs parameter count.\"\"\"\n",
    "    for net_type in [\"EINet\", \"MLP\"]:\n",
    "        if net_type not in results or not results[net_type]:\n",
    "            print(f\"No data for {net_type}\")\n",
    "            continue\n",
    "            \n",
    "        data_list = sorted(results[net_type], key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        if len(data_list) < 2:\n",
    "            print(f\"Not enough data points for {net_type} to calculate scaling exponent\")\n",
    "            continue\n",
    "            \n",
    "        log_params = np.log(np.array([d[\"param_count\"] for d in data_list]))\n",
    "        log_loss = np.log(np.array([max(1e-10, d[\"loss\"]) for d in data_list]))\n",
    "        log_error = np.log(np.array([max(1e-10, 1.0 - d[\"accuracy\"]) for d in data_list]))\n",
    "        \n",
    "        loss_slope, _ = np.polyfit(log_params, log_loss, 1)\n",
    "        error_slope, _ = np.polyfit(log_params, log_error, 1)\n",
    "        \n",
    "        print(f\"\\n{net_type} Scaling:\")\n",
    "        print(f\"  Loss scaling exponent: {loss_slope:.4f}\")\n",
    "        print(f\"  Error scaling exponent: {error_slope:.4f}\")\n",
    "        \n",
    "        if loss_slope < 0:\n",
    "            print(f\"  Loss scales with parameters as: loss ∝ (params)^{loss_slope:.4f}\")\n",
    "            print(f\"  Doubling parameter count decreases loss by {2**abs(loss_slope)-1:.2%}\")\n",
    "        \n",
    "        if error_slope < 0:\n",
    "            print(f\"  Error scales with parameters as: error ∝ (params)^{error_slope:.4f}\")\n",
    "            print(f\"  Doubling parameter count decreases error by {2**abs(error_slope)-1:.2%}\")\n",
    "\n",
    "# Calculate scaling exponents\n",
    "calculate_scaling_exponents(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
