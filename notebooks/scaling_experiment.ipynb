{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:33:42,112 - scaling_notebook - INFO - Added /n/holylabs/LABS/kempner_dev/Users/hsafaai/Code/dendritic-modeling to Python path\n",
      "2025-02-25 10:34:07,045 - scaling_notebook - INFO - Successfully imported train_main from dendritic_modeling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('scaling_notebook')\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if not any(p.endswith('dendritic_modeling') for p in sys.path):\n",
    "    if os.path.basename(current_dir) == 'notebooks':\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added {project_root} to Python path\")\n",
    "\n",
    "try:\n",
    "    from dendritic_modeling.train_experiments import main as train_main\n",
    "    logger.info(\"Successfully imported train_main from dendritic_modeling\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Failed to import: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_yaml(path):\n",
    "    \"\"\"Load a YAML configuration file.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load config file {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_config_yaml(cfg_dict, path):\n",
    "    \"\"\"Save a configuration dictionary to a YAML file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        yaml.dump(cfg_dict, f, sort_keys=False)\n",
    "\n",
    "def parse_experiment_logs(exp_dir):\n",
    "    \"\"\"\n",
    "    Parse param_count and final_loss from experiment logs.\n",
    "    Returns (param_count, final_loss) or (None, None) if unsuccessful.\n",
    "    \"\"\"\n",
    "    final_path = os.path.join(exp_dir, \"final_results.json\")\n",
    "    if not os.path.exists(final_path):\n",
    "        logger.warning(f\"final_results.json not found in {exp_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        with open(final_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        param_count = data.get(\"param_count\")\n",
    "        final_loss = data.get(\"final_loss\")\n",
    "        \n",
    "        # Check if we have valid values\n",
    "        if param_count is None:\n",
    "            logger.warning(f\"param_count not found in {final_path}\")\n",
    "        if final_loss is None:\n",
    "            logger.warning(f\"final_loss not found in {final_path}\")\n",
    "            \n",
    "        return param_count, final_loss\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing {final_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(config_base, output_dir, experiment_name):\n",
    "    \"\"\"\n",
    "    Run a single experiment with the given configuration.\n",
    "    Returns (param_count, final_loss) or (None, None) if unsuccessful.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    config_path = os.path.join(output_dir, \"config.yaml\")\n",
    "    save_config_yaml(config_base, config_path)\n",
    "\n",
    "    logger.info(f\"Running experiment: {experiment_name}\")\n",
    "    try:\n",
    "        train_main(config_path=config_path, output_dir=output_dir, experiment_name=experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Parse experiment logs to get param_count and final_loss\n",
    "    logger.info(f\"Parsing results for: {experiment_name}\")\n",
    "    pcount, loss = parse_experiment_logs(output_dir)\n",
    "    \n",
    "    if pcount is None or loss is None:\n",
    "        logger.warning(f\"Failed to get param_count or final_loss for {experiment_name}\")\n",
    "    else:\n",
    "        logger.info(f\"Successfully retrieved metrics: params={pcount}, loss={loss}\")\n",
    "        \n",
    "    return pcount, loss\n",
    "\n",
    "def json_serialize_fix(obj):\n",
    "    \"\"\"\n",
    "    Helper function to make values JSON serializable.\n",
    "    Handles numpy values, which don't serialize well.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_results_json(results, output_dir, experiment_name_prefix):\n",
    "    \"\"\"\n",
    "    Enhanced function to save results with proper JSON serialization.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy of results to modify\n",
    "        results_copy = {}\n",
    "        \n",
    "        # Process each network type\n",
    "        for net_type, data_list in results.items():\n",
    "            # Process each data point\n",
    "            processed_data_list = []\n",
    "            for data_point in data_list:\n",
    "                # Create a new dict with properly serialized values\n",
    "                processed_data = {}\n",
    "                for key, value in data_point.items():\n",
    "                    processed_data[key] = json_serialize_fix(value)\n",
    "                processed_data_list.append(processed_data)\n",
    "            results_copy[net_type] = processed_data_list\n",
    "        \n",
    "        # Save to file\n",
    "        out_json = os.path.join(output_dir, f\"{experiment_name_prefix}_allresults.json\")\n",
    "        with open(out_json, 'w') as f:\n",
    "            json.dump(results_copy, f, indent=2)\n",
    "            \n",
    "        # Verify by loading back\n",
    "        with open(out_json, 'r') as f:\n",
    "            verification = json.load(f)\n",
    "            \n",
    "        # Log verification info\n",
    "        logger.info(f\"Saved and verified results to {out_json}\")\n",
    "        logger.info(f\"Saved data contains: {list(verification.keys())}\")\n",
    "        for net_type in verification:\n",
    "            logger.info(f\"  {net_type}: {len(verification[net_type])} data points\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_scaling_experiment(base_config_path, output_base_dir, dim_values,\n",
    "                           experiment_name_prefix=\"scaling_exp\"):\n",
    "    \"\"\"\n",
    "    For each dim in dim_values, do 2 runs:\n",
    "      1) EINet\n",
    "      2) MLP\n",
    "    Modify 'excitatory_branch_factors = [2, dim]' in config.\n",
    "    Return a dictionary:\n",
    "      results[\"EINet\"] = list of { \"dim\":..., \"param_count\":..., \"loss\":... }\n",
    "      results[\"MLP\"]   = list of { \"dim\":..., \"param_count\":..., \"loss\":... }\n",
    "    \"\"\"\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "    base_cfg = load_config_yaml(base_config_path)\n",
    "    if not base_cfg:\n",
    "        logger.error(f\"Failed to load base config from {base_config_path}\")\n",
    "        return None\n",
    "\n",
    "    results = {\"EINet\": [], \"MLP\": []}\n",
    "\n",
    "    for dim in dim_values:\n",
    "        for net_type in [\"EINet\", \"MLP\"]:\n",
    "            cfg = deepcopy(base_cfg)\n",
    "\n",
    "            # Set the network type\n",
    "            cfg[\"model\"][\"network\"][\"type\"] = net_type\n",
    "\n",
    "            # Set excitatory_branch_factors = [2, dim]\n",
    "            cfg[\"model\"][\"network\"][\"parameters\"][\"excitatory_branch_factors\"] = [2, int(dim)]\n",
    "\n",
    "            # Ensure we don't have mismatch in inhibitory factors\n",
    "            cfg[\"model\"][\"network\"][\"parameters\"][\"inhibitory_branch_factors\"] = []\n",
    "\n",
    "            # Adjust training settings for faster experiment runs\n",
    "            # Set lower number of epochs (10-20) for this scaling experiment\n",
    "            cfg[\"train\"][\"epochs\"] = 20\n",
    "            \n",
    "            # Build a unique experiment dir\n",
    "            exp_name = f\"{experiment_name_prefix}_{net_type}_dim{dim}\"\n",
    "            exp_dir = os.path.join(output_base_dir, exp_name)\n",
    "\n",
    "            pcount, loss = run_single_experiment(cfg, exp_dir, exp_name)\n",
    "            logger.info(f\"Completed {net_type} dim={dim}: params={pcount}, loss={loss}\")\n",
    "            \n",
    "            if (pcount is not None) and (loss is not None):\n",
    "                results[net_type].append({\"dim\": dim, \"param_count\": pcount, \"loss\": loss})\n",
    "            else:\n",
    "                logger.warning(f\"Could not get param_count/loss for {net_type} dim={dim}\")\n",
    "                \n",
    "                # Try to get param_count manually if possible\n",
    "                try:\n",
    "                    model_path = os.path.join(exp_dir, \"best_model.pt\")\n",
    "                    if os.path.exists(model_path):\n",
    "                        logger.info(f\"Attempting to load model to count parameters: {model_path}\")\n",
    "                        import torch\n",
    "                        model = torch.load(model_path)\n",
    "                        manual_pcount = sum(p.numel() for p in model.parameters())\n",
    "                        logger.info(f\"Manual parameter count: {manual_pcount}\")\n",
    "                        \n",
    "                        # If we have a param count but no loss, use accuracy as fallback\n",
    "                        perf_file = os.path.join(exp_dir, \"performance\", \"final.json\")\n",
    "                        if os.path.exists(perf_file):\n",
    "                            try:\n",
    "                                with open(perf_file, 'r') as f:\n",
    "                                    perf_data = json.load(f)\n",
    "                                test_acc = perf_data.get(\"test accuracy\")\n",
    "                                if test_acc is not None:\n",
    "                                    # Use 1-accuracy as a proxy for loss\n",
    "                                    proxy_loss = 1.0 - float(test_acc)\n",
    "                                    logger.info(f\"Using 1-accuracy as proxy for loss: {proxy_loss}\")\n",
    "                                    \n",
    "                                    results[net_type].append({\n",
    "                                        \"dim\": dim, \n",
    "                                        \"param_count\": manual_pcount, \n",
    "                                        \"loss\": proxy_loss\n",
    "                                    })\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error reading performance file: {e}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed manual parameter counting: {e}\")\n",
    "\n",
    "    # Save the results with enhanced JSON serialization\n",
    "    save_success = save_results_json(results, output_base_dir, experiment_name_prefix)\n",
    "    if not save_success:\n",
    "        logger.warning(\"Results may not have been saved correctly\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglog(results, output_dir, experiment_name_prefix):\n",
    "    \"\"\"\n",
    "    Create a log-log plot of parameter count vs. loss with enhanced error checking.\n",
    "    \"\"\"\n",
    "    # Debug input\n",
    "    logger.info(\"Starting plot_loglog function\")\n",
    "    logger.info(f\"Results type: {type(results)}\")\n",
    "    \n",
    "    if not isinstance(results, dict):\n",
    "        logger.error(f\"Results is not a dictionary: {type(results)}\")\n",
    "        return None\n",
    "        \n",
    "    logger.info(f\"Results keys: {results.keys()}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    has_data = False\n",
    "\n",
    "    # We'll loop over the two network types\n",
    "    for net_type, color, marker in [(\"EINet\", \"blue\", \"o\"), (\"MLP\", \"red\", \"s\")]:\n",
    "        if net_type not in results:\n",
    "            logger.warning(f\"Network type {net_type} not found in results\")\n",
    "            continue\n",
    "            \n",
    "        data_list = results.get(net_type, [])\n",
    "        logger.info(f\"{net_type} data list: {data_list}\")\n",
    "        \n",
    "        # Filter out entries missing param_count or loss\n",
    "        filtered_data = []\n",
    "        for d in data_list:\n",
    "            if not isinstance(d, dict):\n",
    "                logger.warning(f\"Data point is not a dictionary: {d}\")\n",
    "                continue\n",
    "                \n",
    "            if d.get(\"param_count\") and d.get(\"loss\"):\n",
    "                # Ensure values are numeric\n",
    "                try:\n",
    "                    pc = float(d[\"param_count\"])\n",
    "                    loss = float(d[\"loss\"])\n",
    "                    d[\"param_count\"] = pc\n",
    "                    d[\"loss\"] = loss\n",
    "                    filtered_data.append(d)\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    logger.warning(f\"Non-numeric values in data point: {d}, Error: {e}\")\n",
    "            else:\n",
    "                logger.warning(f\"Missing param_count or loss in data point: {d}\")\n",
    "        \n",
    "        data_list = filtered_data\n",
    "        logger.info(f\"{net_type} filtered data list: {data_list}\")\n",
    "        \n",
    "        # Sort by param_count for a nice curve\n",
    "        data_list.sort(key=lambda x: x[\"param_count\"])\n",
    "        \n",
    "        if len(data_list) > 0:\n",
    "            has_data = True\n",
    "            x_param = [d[\"param_count\"] for d in data_list]\n",
    "            y_loss = [d[\"loss\"] for d in data_list]\n",
    "            \n",
    "            logger.info(f\"{net_type} x values: {x_param}\")\n",
    "            logger.info(f\"{net_type} y values: {y_loss}\")\n",
    "            \n",
    "            # Add curve with line between points\n",
    "            ax.plot(x_param, y_loss, marker=marker, color=color, label=net_type, linewidth=2, markersize=10)\n",
    "            \n",
    "            # Add point labels for individual dimensions\n",
    "            for d in data_list:\n",
    "                ax.annotate(\n",
    "                    f\"dim={d['dim']}\", \n",
    "                    xy=(d[\"param_count\"], d[\"loss\"]),\n",
    "                    xytext=(10, 0),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10\n",
    "                )\n",
    "    \n",
    "    if has_data:\n",
    "        ax.set_xlabel(\"Parameter Count (log scale)\", fontsize=12)\n",
    "        ax.set_ylabel(\"Loss (log scale)\", fontsize=12)\n",
    "        ax.set_title(f\"Scaling Behavior: {experiment_name_prefix}\", fontsize=14)\n",
    "        ax.grid(True, which=\"both\", alpha=0.3)\n",
    "        ax.legend(loc='best', fontsize=12)\n",
    "        \n",
    "        # Add minor gridlines\n",
    "        ax.grid(which='minor', linestyle=':', alpha=0.2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        savefig = os.path.join(output_dir, f\"{experiment_name_prefix}_loglog.png\")\n",
    "        plt.savefig(savefig, dpi=300)\n",
    "        logger.info(f\"Saved log-log plot to {savefig}\")\n",
    "        \n",
    "        # Display the plot in the notebook\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.warning(\"No valid data to plot\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def load_existing_results(output_dir, experiment_name_prefix):\n",
    "    \"\"\"\n",
    "    Enhanced function to load existing results with better error handling.\n",
    "    \"\"\"\n",
    "    results_path = os.path.join(output_dir, f\"{experiment_name_prefix}_allresults.json\")\n",
    "    \n",
    "    if not os.path.exists(results_path):\n",
    "        logger.warning(f\"Results file does not exist: {results_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        # Validate structure\n",
    "        if not isinstance(results, dict):\n",
    "            logger.error(f\"Results file has invalid structure - not a dictionary\")\n",
    "            return None\n",
    "            \n",
    "        for net_type in ['EINet', 'MLP']:\n",
    "            if net_type not in results:\n",
    "                logger.warning(f\"Results file missing network type: {net_type}\")\n",
    "                results[net_type] = []\n",
    "                \n",
    "            if not isinstance(results[net_type], list):\n",
    "                logger.error(f\"Results for {net_type} is not a list\")\n",
    "                results[net_type] = []\n",
    "        \n",
    "        logger.info(f\"Successfully loaded results from {results_path}\")\n",
    "        # Log what was loaded\n",
    "        for net_type in results:\n",
    "            logger.info(f\"  {net_type}: {len(results[net_type])} data points\")\n",
    "            for item in results[net_type]:\n",
    "                logger.info(f\"    dim={item.get('dim')}, params={item.get('param_count')}, loss={item.get('loss')}\")\n",
    "                \n",
    "        return results\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Results file contains invalid JSON: {results_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading results: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = \"../.vscode/config_exp.yaml\"  # Path to base configuration file\n",
    "OUTPUT_DIR = \"/n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment\"  # Directory to save results\n",
    "DIM_LIST = [2, 4, 8, 16, 32]  # Dimensions to sweep through\n",
    "EXPERIMENT_NAME = \"scaling_exp\"  # Prefix for experiment names\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:34:07,260 - scaling_notebook - INFO - Successfully loaded results from /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_allresults.json\n",
      "2025-02-25 10:34:07,261 - scaling_notebook - INFO -   EINet: 0 data points\n",
      "2025-02-25 10:34:07,261 - scaling_notebook - INFO -   MLP: 0 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing results from previous runs.\n",
      "\n",
      "Existing Results Summary:\n",
      "\n",
      "EINet Results:\n",
      "\n",
      "MLP Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:05:30,934 - scaling_notebook - INFO - Running experiment: scaling_exp_EINet_dim2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running scaling experiments...\n",
      "INFO logging_config.py:52  Log file set to /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_EINet_dim2/dendritic_modeling.log.\n",
      "INFO train_experiments.py:102  Loading dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO train_experiments.py:55  Running EINet with probabilistic classifier\n",
      "INFO train_experiments.py:139  Using learning strategy: mle\n",
      "INFO train_experiments.py:152  Saving results and visualizations to: /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_EINet_dim2_2025-02-25_11-05-40\n",
      "INFO train_experiments.py:188  Completed epoch 1/20\n",
      "INFO train_experiments.py:188  Completed epoch 2/20\n",
      "INFO train_experiments.py:188  Completed epoch 3/20\n",
      "INFO train_experiments.py:188  Completed epoch 4/20\n",
      "INFO train_experiments.py:188  Completed epoch 5/20\n",
      "INFO train_experiments.py:188  Completed epoch 6/20\n",
      "INFO train_experiments.py:188  Completed epoch 7/20\n",
      "INFO train_experiments.py:188  Completed epoch 8/20\n",
      "INFO train_experiments.py:188  Completed epoch 9/20\n",
      "INFO train_experiments.py:188  Completed epoch 10/20\n",
      "INFO train_experiments.py:188  Completed epoch 11/20\n",
      "INFO train_experiments.py:188  Completed epoch 12/20\n",
      "INFO train_experiments.py:188  Completed epoch 13/20\n",
      "INFO train_experiments.py:188  Completed epoch 14/20\n",
      "INFO train_experiments.py:188  Completed epoch 15/20\n",
      "INFO train_experiments.py:188  Completed epoch 16/20\n",
      "INFO train_experiments.py:188  Completed epoch 17/20\n",
      "INFO train_experiments.py:188  Completed epoch 18/20\n",
      "INFO train_experiments.py:188  Completed epoch 19/20\n",
      "INFO train_experiments.py:188  Completed epoch 20/20\n",
      "INFO train_experiments.py:265  Evaluating performance...\n",
      "INFO train_experiments.py:274  train acc=0.231, valid acc=0.230, test acc=0.237\n",
      "INFO train_experiments.py:287  Visualization disabled; final plotting skipped.\n",
      "INFO train_experiments.py:289  All done.\n",
      "INFO train_experiments.py:301  Saved final_results.json => /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_EINet_dim2_2025-02-25_11-05-40/final_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:09:37,370 - scaling_notebook - INFO - Parsing results for: scaling_exp_EINet_dim2\n",
      "2025-02-25 11:09:37,370 - scaling_notebook - WARNING - final_results.json not found in /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_EINet_dim2\n",
      "2025-02-25 11:09:37,370 - scaling_notebook - WARNING - Failed to get param_count or final_loss for scaling_exp_EINet_dim2\n",
      "2025-02-25 11:09:37,370 - scaling_notebook - INFO - Completed EINet dim=2: params=None, loss=None\n",
      "2025-02-25 11:09:37,371 - scaling_notebook - WARNING - Could not get param_count/loss for EINet dim=2\n",
      "2025-02-25 11:09:37,374 - scaling_notebook - INFO - Running experiment: scaling_exp_MLP_dim2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO logging_config.py:52  Log file set to /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_MLP_dim2/dendritic_modeling.log.\n",
      "INFO train_experiments.py:102  Loading dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO train_experiments.py:46  Running feedforward with probabilistic classifier\n",
      "INFO networks.py:341  Building MLPExcInhNetwork...\n",
      "INFO train_experiments.py:139  Using learning strategy: mle\n",
      "INFO train_experiments.py:152  Saving results and visualizations to: /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_MLP_dim2_2025-02-25_11-09-42\n",
      "INFO train_experiments.py:188  Completed epoch 1/20\n",
      "INFO train_experiments.py:188  Completed epoch 2/20\n",
      "INFO train_experiments.py:188  Completed epoch 3/20\n",
      "INFO train_experiments.py:188  Completed epoch 4/20\n",
      "INFO train_experiments.py:188  Completed epoch 5/20\n",
      "INFO train_experiments.py:188  Completed epoch 6/20\n",
      "INFO train_experiments.py:188  Completed epoch 7/20\n",
      "INFO train_experiments.py:188  Completed epoch 8/20\n",
      "INFO train_experiments.py:188  Completed epoch 9/20\n",
      "INFO train_experiments.py:188  Completed epoch 10/20\n",
      "INFO train_experiments.py:188  Completed epoch 11/20\n",
      "INFO train_experiments.py:188  Completed epoch 12/20\n",
      "INFO train_experiments.py:188  Completed epoch 13/20\n",
      "INFO train_experiments.py:188  Completed epoch 14/20\n",
      "INFO train_experiments.py:188  Completed epoch 15/20\n",
      "INFO train_experiments.py:188  Completed epoch 16/20\n",
      "INFO train_experiments.py:188  Completed epoch 17/20\n",
      "INFO train_experiments.py:188  Completed epoch 18/20\n",
      "INFO train_experiments.py:188  Completed epoch 19/20\n",
      "INFO train_experiments.py:188  Completed epoch 20/20\n",
      "INFO train_experiments.py:265  Evaluating performance...\n",
      "INFO train_experiments.py:274  train acc=0.106, valid acc=0.098, test acc=0.105\n",
      "INFO train_experiments.py:287  Visualization disabled; final plotting skipped.\n",
      "INFO train_experiments.py:289  All done.\n",
      "INFO train_experiments.py:301  Saved final_results.json => /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_MLP_dim2_2025-02-25_11-09-42/final_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:10:16,669 - scaling_notebook - INFO - Parsing results for: scaling_exp_MLP_dim2\n",
      "2025-02-25 11:10:16,669 - scaling_notebook - WARNING - final_results.json not found in /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_MLP_dim2\n",
      "2025-02-25 11:10:16,670 - scaling_notebook - WARNING - Failed to get param_count or final_loss for scaling_exp_MLP_dim2\n",
      "2025-02-25 11:10:16,670 - scaling_notebook - INFO - Completed MLP dim=2: params=None, loss=None\n",
      "2025-02-25 11:10:16,670 - scaling_notebook - WARNING - Could not get param_count/loss for MLP dim=2\n",
      "2025-02-25 11:10:16,673 - scaling_notebook - INFO - Running experiment: scaling_exp_EINet_dim4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO logging_config.py:52  Log file set to /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_EINet_dim4/dendritic_modeling.log.\n",
      "INFO train_experiments.py:102  Loading dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO train_experiments.py:55  Running EINet with probabilistic classifier\n",
      "INFO train_experiments.py:139  Using learning strategy: mle\n",
      "INFO train_experiments.py:152  Saving results and visualizations to: /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_EINet_dim4_2025-02-25_11-10-21\n",
      "INFO train_experiments.py:188  Completed epoch 1/20\n",
      "INFO train_experiments.py:188  Completed epoch 2/20\n",
      "INFO train_experiments.py:188  Completed epoch 3/20\n",
      "INFO train_experiments.py:188  Completed epoch 4/20\n",
      "INFO train_experiments.py:188  Completed epoch 5/20\n",
      "INFO train_experiments.py:188  Completed epoch 6/20\n",
      "INFO train_experiments.py:188  Completed epoch 7/20\n",
      "INFO train_experiments.py:188  Completed epoch 8/20\n",
      "INFO train_experiments.py:188  Completed epoch 9/20\n",
      "INFO train_experiments.py:188  Completed epoch 10/20\n",
      "INFO train_experiments.py:188  Completed epoch 11/20\n",
      "INFO train_experiments.py:188  Completed epoch 12/20\n",
      "INFO train_experiments.py:188  Completed epoch 13/20\n",
      "INFO train_experiments.py:188  Completed epoch 14/20\n",
      "INFO train_experiments.py:188  Completed epoch 15/20\n",
      "INFO train_experiments.py:188  Completed epoch 16/20\n",
      "INFO train_experiments.py:188  Completed epoch 17/20\n",
      "INFO train_experiments.py:188  Completed epoch 18/20\n",
      "INFO train_experiments.py:188  Completed epoch 19/20\n",
      "INFO train_experiments.py:188  Completed epoch 20/20\n",
      "INFO train_experiments.py:265  Evaluating performance...\n",
      "INFO train_experiments.py:274  train acc=0.275, valid acc=0.277, test acc=0.272\n",
      "INFO train_experiments.py:287  Visualization disabled; final plotting skipped.\n",
      "INFO train_experiments.py:289  All done.\n",
      "INFO train_experiments.py:301  Saved final_results.json => /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_EINet_dim4_2025-02-25_11-10-21/final_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:14:04,209 - scaling_notebook - INFO - Parsing results for: scaling_exp_EINet_dim4\n",
      "2025-02-25 11:14:04,209 - scaling_notebook - WARNING - final_results.json not found in /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_EINet_dim4\n",
      "2025-02-25 11:14:04,209 - scaling_notebook - WARNING - Failed to get param_count or final_loss for scaling_exp_EINet_dim4\n",
      "2025-02-25 11:14:04,209 - scaling_notebook - INFO - Completed EINet dim=4: params=None, loss=None\n",
      "2025-02-25 11:14:04,210 - scaling_notebook - WARNING - Could not get param_count/loss for EINet dim=4\n",
      "2025-02-25 11:14:04,213 - scaling_notebook - INFO - Running experiment: scaling_exp_MLP_dim4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO logging_config.py:52  Log file set to /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_MLP_dim4/dendritic_modeling.log.\n",
      "INFO train_experiments.py:102  Loading dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO train_experiments.py:46  Running feedforward with probabilistic classifier\n",
      "INFO networks.py:341  Building MLPExcInhNetwork...\n",
      "INFO train_experiments.py:139  Using learning strategy: mle\n",
      "INFO train_experiments.py:152  Saving results and visualizations to: /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_MLP_dim4_2025-02-25_11-14-09\n",
      "INFO train_experiments.py:188  Completed epoch 1/20\n",
      "INFO train_experiments.py:188  Completed epoch 2/20\n",
      "INFO train_experiments.py:188  Completed epoch 3/20\n",
      "INFO train_experiments.py:188  Completed epoch 4/20\n",
      "INFO train_experiments.py:188  Completed epoch 5/20\n",
      "INFO train_experiments.py:188  Completed epoch 6/20\n",
      "INFO train_experiments.py:188  Completed epoch 7/20\n",
      "INFO train_experiments.py:188  Completed epoch 8/20\n",
      "INFO train_experiments.py:188  Completed epoch 9/20\n",
      "INFO train_experiments.py:188  Completed epoch 10/20\n",
      "INFO train_experiments.py:188  Completed epoch 11/20\n",
      "INFO train_experiments.py:188  Completed epoch 12/20\n",
      "INFO train_experiments.py:188  Completed epoch 13/20\n",
      "INFO train_experiments.py:188  Completed epoch 14/20\n",
      "INFO train_experiments.py:188  Completed epoch 15/20\n",
      "INFO train_experiments.py:188  Completed epoch 16/20\n",
      "INFO train_experiments.py:188  Completed epoch 17/20\n",
      "INFO train_experiments.py:188  Completed epoch 18/20\n",
      "INFO train_experiments.py:188  Completed epoch 19/20\n",
      "INFO train_experiments.py:188  Completed epoch 20/20\n",
      "INFO train_experiments.py:265  Evaluating performance...\n",
      "INFO train_experiments.py:274  train acc=0.102, valid acc=0.100, test acc=0.102\n",
      "INFO train_experiments.py:287  Visualization disabled; final plotting skipped.\n",
      "INFO train_experiments.py:289  All done.\n",
      "INFO train_experiments.py:301  Saved final_results.json => /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_MLP_dim4_2025-02-25_11-14-09/final_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:14:43,872 - scaling_notebook - INFO - Parsing results for: scaling_exp_MLP_dim4\n",
      "2025-02-25 11:14:43,872 - scaling_notebook - WARNING - final_results.json not found in /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_MLP_dim4\n",
      "2025-02-25 11:14:43,872 - scaling_notebook - WARNING - Failed to get param_count or final_loss for scaling_exp_MLP_dim4\n",
      "2025-02-25 11:14:43,873 - scaling_notebook - INFO - Completed MLP dim=4: params=None, loss=None\n",
      "2025-02-25 11:14:43,873 - scaling_notebook - WARNING - Could not get param_count/loss for MLP dim=4\n",
      "2025-02-25 11:14:43,876 - scaling_notebook - INFO - Running experiment: scaling_exp_EINet_dim8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO logging_config.py:52  Log file set to /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/scaling_experiment/scaling_exp_EINet_dim8/dendritic_modeling.log.\n",
      "INFO train_experiments.py:102  Loading dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO train_experiments.py:55  Running EINet with probabilistic classifier\n",
      "INFO train_experiments.py:139  Using learning strategy: mle\n",
      "INFO train_experiments.py:152  Saving results and visualizations to: /n/holylabs/LABS/kempner_dev/Users/hsafaai/results/dendritic_modeling/results/scaling_exp_EINet_dim8_2025-02-25_11-14-48\n",
      "INFO train_experiments.py:188  Completed epoch 1/20\n",
      "INFO train_experiments.py:188  Completed epoch 2/20\n",
      "INFO train_experiments.py:188  Completed epoch 3/20\n",
      "INFO train_experiments.py:188  Completed epoch 4/20\n",
      "INFO train_experiments.py:188  Completed epoch 5/20\n"
     ]
    }
   ],
   "source": [
    "existing_results = load_existing_results(OUTPUT_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "print(\"No existing results found. Running scaling experiments...\")\n",
    "results = run_scaling_experiment(\n",
    "    base_config_path=BASE_CONFIG,\n",
    "    output_base_dir=OUTPUT_DIR,\n",
    "    dim_values=DIM_LIST,\n",
    "    experiment_name_prefix=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "if results:\n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_loglog(results, OUTPUT_DIR, EXPERIMENT_NAME)\n",
    "    print(\"\\nScaling experiment completed successfully!\")\n",
    "else:\n",
    "    print(\"\\nNo results to plot. Check logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    for net_type in [\"EINet\", \"MLP\"]:\n",
    "        data_list = results.get(net_type, [])\n",
    "        # Filter out entries missing param_count or loss\n",
    "        data_list = [d for d in data_list if (d.get(\"param_count\") and d.get(\"loss\"))]\n",
    "        \n",
    "        if len(data_list) >= 2:  # Need at least 2 points to calculate slope\n",
    "            # Sort by param_count\n",
    "            data_list.sort(key=lambda x: x[\"param_count\"])\n",
    "            \n",
    "            # Extract log values\n",
    "            log_params = np.log(np.array([d[\"param_count\"] for d in data_list]))\n",
    "            log_loss = np.log(np.array([d[\"loss\"] for d in data_list]))\n",
    "            \n",
    "            # Simple linear regression to find slope\n",
    "            if len(log_params) > 1:  # Check for valid array length\n",
    "                slope = np.polyfit(log_params, log_loss, 1)[0]\n",
    "                print(f\"\\n{net_type} scaling exponent (slope in log-log space): {slope:.4f}\")\n",
    "                \n",
    "                if slope < 0:\n",
    "                    print(f\"Loss scales with parameters as: loss ∝ (params)^{slope:.4f}\")\n",
    "                    print(f\"This means that doubling parameter count decreases loss by {2**abs(slope)-1:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dendritic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
